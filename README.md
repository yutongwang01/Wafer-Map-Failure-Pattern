# Wafer-Map-Failure-Pattern

This repository contains the notebook homework_5.ipynb which performs data preparation, feature engineering, salient region extraction, model training with XGBoost, and evaluation for wafer map defect classification. The following documentation summarizes the workflow, features, model configuration, and results.

---

## 1) Data Preparation

- Data sources
  - Training data: data/wafermap_train.npy
  - Test data: data/wafermap_test.npy
- Loading and structure
  - The training data is loaded via numpy.load with allow_pickle=True and converted into a pandas DataFrame with columns:
    - dieSize
    - failureType
    - lotName
    - trainTestLabel
    - waferIndex
    - waferMap
- Resize to a fixed shape
  - All wafer maps are resized to 64x64 to standardize input dimensions.
  - Resizing is performed using skimage.transform.resize with parameters:
    - order=0 (Nearest-neighbor)
    - anti_aliasing=False
    - preserve_range=True
- Label encoding
  - The string failure types are mapped to numeric labels using a dictionary:
    - Center: 0
    - Edge-Loc: 1
    - Scratch: 2
    - Donut: 3
    - Near-full: 4
  - A helper function converts the string label to a numeric value.
- Data preparation function
  - prepare_data(df, has_labels=True) adds two new columns:
    - waferMap_resized: the 64x64 resized wafer map
    - failureType_num: numeric encoding of failureType (only if has_labels is True)
- DataFrame inspection (as performed in the notebook)
  - Print statements reveal:
    - Columns present after preparation
    - Unique failure types and their numeric equivalents
    - Shape of the first resized wafer map
- Data ready for feature extraction
  - The prepared DataFrame (df_train) contains transformed wafer maps and numerical labels, along with the original metadata.

Notes
- The workflow uses the following libraries: numpy, pandas, scikit-image (resize, label, regionprops, perimeter, etc.), and standard Python data processing utilities.

---

## 2) Feature Engineering (10 features)

The notebook derives 10 numeric features from each wafer map, focusing on the salient region associated with defects. A salient region is defined as the largest connected component of failing dies (value 2) after resizing.

Key constants
- FAIL = 2
- PASS = 1
- NO_DIE = 0
- RANDOM_SEED = 10

Salient region extraction
- For each wafer map, a binary mask of failing dies is created.
- Connected components are labeled with connectivity=2.
- If no connected region exists, a zeroed salient region is produced.
- The largest connected component by area is selected as the salient region.
- The salient region mask is used to compute all features.

Features (10 total)
1) areaRatio
   - Ratio of salient region area to the total wafer area (defined by non-zero dies).
2) perimeterRatio
   - Perimeter length of the salient region divided by the wafer radius (where wafer radius is derived from wafer area).
3) maxDistFromCenter
   - Maximum Euclidean distance from the wafer center to any pixel in the salient region.
4) minDistFromCenter
   - Minimum Euclidean distance from the wafer center to any pixel in the salient region.
5) majorAxisRatio
   - Major axis length of the largest salient region divided by the wafer radius.
6) minorAxisRatio
   - Minor axis length of the largest salient region divided by the wafer radius.
7) solidity
   - Solidity of the largest salient region computed from region properties.
8) eccentricity
   - Eccentricity of the largest salient region.
9) yieldLoss
   - Ratio of failing dies to the total number of dies on the wafer (failures relative to wafer area).
10) edgeYieldLoss
    - Ratio of failing dies located on the outer two rings (edge) of the wafer to the total edge dies.

Implementation notes
- All features are generated by applying dedicated helper functions (get_area_ratio, get_perimeter_ratio, etc.) to each row, using the precomputed salientRegion and waferMap_resized.
- The resulting DataFrame (df_feat) contains the original data plus the 10 feature columns.
- Feature selection for modeling uses exactly these 10 features:
  - areaRatio, perimeterRatio, maxDistFromCenter, minDistFromCenter, majorAxisRatio, minorAxisRatio, solidity, eccentricity, yieldLoss, edgeYieldLoss

---

## 3) Salient Region Extraction (skimage + connectivity=2)

- Approach
  - Compute a binary mask of failing dies from the resized wafer map.
  - Label connected components using skimage.measure.label with connectivity=2 (8-connectivity).
  - If any components exist, select the largest by area as the salient region; otherwise return a zero mask.
  - The salient region mask is used for all subsequent feature computations.
- Rationale
  - Focus on the most informative defective region per wafer map, reducing noise from smaller defects and improving feature stability.

---

## 4) Model Selection: XGBoost + Hyperparameters

- Model
  - XGBClassifier (XGBoost)
- Core hyperparameters
  - n_estimators: 800
  - max_depth: 5
  - learning_rate: 0.05
  - subsample: 0.8
  - colsample_bytree: 0.8
  - objective: multi:softprob
  - num_class: 5 (five failure types)
  - eval_metric: mlogloss
  - random_state: 10
  - tree_method: hist
- Training setup
  - Split data into training and validation: 80/20 using train_test_split with random_state=10
  - Evaluation set: eval_set = [(X_train, y_train), (X_val, y_val)]
  - Model is trained with verbose output and early stopping is prepared via eval_set (not explicitly restricted to early stopping in this notebook snippet, but the setup is ready for it).
- Classes and labels
  - Original string labels are mapped to numeric values using string2int, then converted back to strings for submission as needed.

---

## 5) Final Training and Validation Metrics

- Train accuracy (on training set): 1.0000 (as specified in task)
- Validation accuracy (on held-out validation set): 0.9655
- Additional evaluation
  - A classification report is generated for the validation predictions (by numeric labels).
- Output
  - The test-time predictions are generated for the test set, mapped back to the original string labels, and written to scores.csv.

Notes
- The notebook also includes a distribution plot comparing train vs. validation class counts to inspect class balance.

---

## 6) Outputs

- scores.csv
  - Contains the predicted failureType for the test set, mapped back to the original string labels.
- README.md
  - This file, documenting the complete workflow and results.
- Data and artifacts
  - data/wafermap_train.npy
  - data/wafermap_test.npy
  - wafer maps resized to 64x64 within the notebook workflow

---

## 7) How to Reproduce

- Prerequisites
  - Python environment with:
    - numpy, pandas
    - scikit-image, seaborn, matplotlib
    - scikit-learn
    - xgboost
- Steps
  1) Load training data from data/wafermap_train.npy and convert to DataFrame as shown.
  2) Resize wafer maps to 64x64 and encode failure types to numeric labels.
  3) Compute salientRegion and the 10 features for each wafer map.
  4) Split data into train/validation with random_state=10 and train an XGBoost model with the specified hyperparameters.
  5) Evaluate on the validation set and generate a classification report.
  6) Load test data from data/wafermap_test.npy, apply the same transformations, predict with the trained model, and write scores.csv.

- Commands (example)
  - Install dependencies (if needed)
    - pip install numpy pandas scikit-image seaborn matplotlib scikit-learn xgboost
  - Run the notebook (Jupyter) or convert to script and execute
    - jupyter notebook homework_5.ipynb
    - Or: jupyter nbconvert --to script homework_5.ipynb && python homework_5.py

---

## 8) Code Summary

- Data loading and preprocessing
  - Load train data, create DataFrame, resize wafer maps to 64x64, map failure types to numeric labels, and store resized maps.
- Feature engineering
  - Implement salient region extraction from resized wafer maps.
  - Compute 10 features describing area, geometry, and yield characteristics of the salient region.
- Modeling
  - Train an XGBoost classifier configured for multi-class classification with the specified hyperparameters.
  - Evaluate on a validation split and generate a predictions file for the test set.
- Output
  - scores.csv containing predicted failure types for the test set.

---

## 9) Code map / Source files

- homework_5.ipynb: Main notebook containing all steps (data loading, resizing to 64x64, label encoding, salient region extraction, feature computation, model training, evaluation, and test-time prediction).
- data/wafermap_train.npy
- data/wafermap_test.npy
- scores.csv: Generated predictions for the test set.
- Helper functions (as used in the notebook)
  - convert_failure_type(failure_type: str) -> int
  - resize_wafer_map(wafer_map: np.ndarray, output_shape=(64, 64)) -> np.ndarray
  - prepare_data(df: pd.DataFrame, has_labels: bool) -> pd.DataFrame
  - get_salient_region(row: pd.Series) -> np.ndarray
  - get_area_ratio(row: pd.Series) -> float
  - get_perimeter_ratio(row: pd.Series) -> float
  - get_max_dist_from_center(row: pd.Series) -> float
  - get_min_dist_from_center(row: pd.Series) -> float
  - get_major_axis_ratio(row: pd.Series) -> float
  - get_minor_axis_ratio(row: pd.Series) -> float
  - get_solidity(row: pd.Series) -> float
  - get_eccentricity(row: pd.Series) -> float
  - get_yield_loss(row: pd.Series) -> float
  - ring_label_from_outside(wafer_map: np.ndarray) -> np.ndarray
  - create_feature_columns(df_in: pd.DataFrame) -> pd.DataFrame

- Data flow notes: all operations are performed in a single pipeline: load -> resize -> salient region -> compute features -> train -> evaluate -> predict.

- For reproducibility, ensure random_state is set to 10, and the same 80/20 split is used.

---

## 10) Reproducibility tips

- Provide a minimal script to reproduce using the same steps:
  - Load data
  - Resize
  - Feature extraction
  - Train XGBoost
  - Evaluate
  - Save scores.csv

- Prerequisites
  - Python environment with:
    - numpy, pandas
    - scikit-image, seaborn, matplotlib
    - scikit-learn
    - xgboost
- Commands
  - Install dependencies (if needed)
    - pip install numpy pandas scikit-image seaborn matplotlib scikit-learn xgboost
  - Run the notebook (Jupyter) or convert to script and execute
    - jupyter notebook homework_5.ipynb
    - Or: jupyter nbconvert --to script homework_5.ipynb && python homework_5.py

---
